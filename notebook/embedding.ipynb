{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a17b64-be92-457e-8297-0b129ab12da9",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a722dc22-9a81-4d79-8d6d-869abdcba785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text size = 20479 token size = 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "tokens = tokenizer.encode(raw_text)\n",
    "print(f\"raw text size = {len(raw_text)} token size = {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a539c28-5f04-4538-895c-efc8c3e3d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_dataset_v1 import GPTDatasetV1, create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f119fe-d14f-4178-825c-3700a0b0176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 5141\n",
      "Dataset[50] = (tensor([ 290, 4920, 2241,  287]), tensor([4920, 2241,  287,  257]))\n"
     ]
    }
   ],
   "source": [
    "ds = GPTDatasetV1(tiktoken.get_encoding('gpt2'), raw_text, 4, 1)\n",
    "print(f\"Dataset size = {len(ds)}\")\n",
    "print(f\"Dataset[50] = {ds[50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f15361-ad2a-4050-85b0-5df82ecf67d9",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   40,   367,  2885,  1464],\n",
       "         [10899,  2138,   257,  7026],\n",
       "         [  922,  5891,  1576,   438],\n",
       "         [ 1049,  5975,   284,   502],\n",
       "         [  287,   262,  6001,   286],\n",
       "         [  550,  5710,   465, 12036],\n",
       "         [27075,    11,   290,  4920],\n",
       "         [   64,   319,   262, 34686]]),\n",
       " tensor([[  367,  2885,  1464,  1807],\n",
       "         [ 2138,   257,  7026, 15632],\n",
       "         [ 5891,  1576,   438,   568],\n",
       "         [ 5975,   284,   502,   284],\n",
       "         [  262,  6001,   286,   465],\n",
       "         [ 5710,   465, 12036,    11],\n",
       "         [   11,   290,  4920,  2241],\n",
       "         [  319,   262, 34686, 41976]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 4\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    stride=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e30fff-c66f-477a-9f5d-584ea1e1ca94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "EMBEDDING_DIM = 4\n",
    "input_embedding_layer = torch.nn.Embedding(tokenizer.n_vocab, EMBEDDING_DIM)\n",
    "pos_embedding_layer = torch.nn.Embedding(CONTEXT_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bdbbfc8-0fb0-4c0a-857f-000d09b12b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9220,  0.3543, -1.7453, -0.0172],\n",
       "        [-0.0062,  0.5936,  0.4708,  0.8263],\n",
       "        [-1.2343,  1.3128, -0.9582, -1.5700],\n",
       "        [-0.6964,  1.3787,  1.7570, -2.0223]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = input_embedding_layer(first_batch[0])\n",
    "input_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b5d2464-0556-4fd4-a550-99d152c7bd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5441, -0.0155, -0.3817,  0.2478],\n",
       "        [-0.5039,  0.2037,  1.7721,  0.5258],\n",
       "        [ 0.4856, -1.0139, -1.4608,  0.4787],\n",
       "        [ 0.7816, -1.5947, -0.1290, -0.1381]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(CONTEXT_SIZE))\n",
    "pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "929b3328-9e44-4690-9a06-66b5a825c0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4660,  0.3388, -2.1270,  0.2306],\n",
       "        [-0.5101,  0.7974,  2.2429,  1.3521],\n",
       "        [-0.7487,  0.2989, -2.4190, -1.0913],\n",
       "        [ 0.0853, -0.2159,  1.6280, -2.1604]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embedding = input_embedding + pos_embedding\n",
    "final_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "983d1860-bc19-4588-86a4-935be1bbb86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4661"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.922-1.5441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d95e7d-d208-4e18-8b1c-214f2a079cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
